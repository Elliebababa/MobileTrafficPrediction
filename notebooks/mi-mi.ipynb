{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,10000])\n",
    "W = tf.Variable(tf.zeros([784,10000]))\n",
    "b = tf.Variable(tf.zeros([10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_= tf.placeholder(tf.float32,[None,10000])\n",
    "loss = tf.losses.mean_squared_error(labels,prediction)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.initiallize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = \n",
    "    sess.run(train_step, feed_dict={x:batch_xs, y_:batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(labels,prediction)\n",
    "print(sess.run(loss,feed_dict = {x:x_test,y_:y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_model(object):\n",
    "    def  __init__(self):\n",
    "        self.regularizes = []\n",
    "    \n",
    "    def build_graph(self,featureNum):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            #Inputs\n",
    "            with tf.name_scope('input'):\n",
    "                self.ph_data = tf.placeholder(tf.float32, (self.batch_size,featureNum),'data')\n",
    "                self.ph_t_values = tf.placeholder(tf.float32,(self.batch_size,10000),'t_values')\n",
    "                self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n",
    "            #Model\n",
    "            op_values = self.inference(self.ph_data,self.ph_dropout)\n",
    "            self.op_loss = self.loss(op_values, self.t_values, self.regularization)\n",
    "            self.op_train = self.training(self.op_loss, self.learning_rate, self.decay_steps, self.decay_rate, self.momentum)\n",
    "            self.op_prediction = self.prediction(op_values)\n",
    "        \n",
    "            #initialize variables, i.e.. weights and biases\n",
    "            self.op_init = tf.global_variables_initializer()\n",
    "            \n",
    "            #summaries for tensorboard and save for model parameters\n",
    "            self.op_summary = tf.summary.merge_all()\n",
    "            self.op_saver = tf.train.Saver(max_to_keep = 5)\n",
    "        self.graph.finalize()\n",
    "        \n",
    "    def inference(self, data, dropout):\n",
    "        #it builds the computational graph, as far as is required for running the network forward to make prediciton \n",
    "        #i.e. return the lofits given raw data\n",
    "        #data  size N*M\n",
    "        # N:number  of signals(samples)\n",
    "        # M:number of features\n",
    "        values = self._inference(data, dropout)\n",
    "        return values\n",
    "           \n",
    "    def loss(self, values, t_values, regularizarion):\n",
    "        # adds to the inference model the layers required to generate loss\n",
    "        with tf.name_scope('loss'):\n",
    "            with tf.name_scope('mse'):\n",
    "                mse = tf.square(values - t_values)\n",
    "                mse = tf.reduce_mean(mse)\n",
    "            with tf.name_scope('reularizarion'):\n",
    "                regularization *= tf.add_n(self.regularizers)\n",
    "            loss = cross_entropy + regularization\n",
    "            \n",
    "        #summaries for tensorboard\n",
    "        tf.summary.scalar('loss/mse',mse)\n",
    "        tf.summary.scalar('loss/regularization',regularization)\n",
    "        tf.summary.scalar('loss/total',loss)\n",
    "        with tf.name_scope('averages'):\n",
    "            #calculate the average loss up to now\n",
    "            pass\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def training(self,loss, learning_rate, decay_steps, decay_rate = 0.95, momentum = 0.9):\n",
    "        with tf.name_scope('training'):\n",
    "            #learning rate\n",
    "            global_step = tf.Variable(0, name = 'global_step',trainable = False)\n",
    "            if decay_rate != 1:\n",
    "                learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase = True)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            #optimizer\n",
    "            if momentum == 0:\n",
    "                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            else :\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "            grads = optimizer.compute_gradients(loss)\n",
    "            op_gradients = optimizer.apply_gradients(grads, global_step = global_step)\n",
    "            #Histograms\n",
    "            for grad ,var in grads:\n",
    "                if grad is None:\n",
    "                    print('warning:{} has no gradient'.format(var.op.name))\n",
    "                else:\n",
    "                    tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "            #The op return the learning rate\n",
    "            with tf.control_dependencies([op_gradients]):\n",
    "                op_train = tf.identity(learning_rate, name = 'control')\n",
    "            return op_train\n",
    "    \n",
    "    def _conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x,W,strides = [1,1,1,1], padding = 'SAME')\n",
    "    \n",
    "    def predict(self, data, labels = None, sess = None):\n",
    "        loss = 0\n",
    "        size = data.shape[0]\n",
    "        predictions = np.empty(size)\n",
    "        sess = self._get_session(sess)\n",
    "        for begin in range(0,size,self.batch_size):\n",
    "            end = begin + self.batch_size\n",
    "            end = min([end,size])\n",
    "            \n",
    "            batch_data = np.zeros((self.batch_size, data.shape[1]))\n",
    "            tmp_data = data[begin:end,:]\n",
    "            #convert sparse matrices\n",
    "            if type(tmp_data) is not np.ndarray():\n",
    "                tmp_data = tmp.data.toarray()\n",
    "            batch_data[:end - begin] = tmp_data\n",
    "            feed_dict = {self.ph_data: batch_data,self.ph_dropout:1}\n",
    "            \n",
    "            #compute the loss\n",
    "            if labels is not None:\n",
    "                batch_labels = np.zeros(self.batch_size)\n",
    "                batch_labels[:end-begin] = labels[begin:end]\n",
    "    \n",
    "    def fit(self, train_data, train_y, val_data, val_labels):\n",
    "        #process time\n",
    "        t_process, t_wall = time.process_time(), time.time()\n",
    "        sess = tf.Session(graph = self.graph)\n",
    "        \n",
    "        #logging the fit information\n",
    "        #shutil.rmtree(self._get_path('summaries'), ignore_errors = True)\n",
    "        #writer = tf.summary.FileWriter(self._get_path('summaries'),self.graph)\n",
    "        #shutil.rmtree(self._get_path('checkpoints'), ignore_errors = True)\n",
    "        #os.makedirs(self._get_path('checkpoints'))\n",
    "        #path = os.path.join(self._get_path('checkpoints'),'model')\n",
    "        sess.run(self.op_init)\n",
    "        #Training\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        indices = collections.deque()\n",
    "        num_steps = int(self.num_epochs * train_data.shape[0]/self.batch_size)\n",
    "        for step in range(1, num_steps+1):\n",
    "            if len(indices) < self.batch_size:\n",
    "                indices.extend(np.random.permutation(train_data.shape[0]))\n",
    "            idx = [indices.popleft() for i in range(self.batch_size)]\n",
    "            batch_size, batch_labels = train_data[idx,i],train_labels[idx]\n",
    "            if type(batch_data) is not np.ndarray:\n",
    "                batch_data = batch_data.toarray() # comvert sparse matrices\n",
    "            feed_dict = {self.ph_data : batch_data, self.ph_labels:batch_labels, self.ph_dropout:self.dropout}\n",
    "            learning_rate, loss_average = sees.run([self.op_train,self.op_loss_average],feed_dict)\n",
    "        \n",
    "         #periodical evaluation of the model\n",
    "            if step % self.eval_frequency == 0 or step == num_steps:\n",
    "                epoch = step * self.batch_size / train_data.shape[0]\n",
    "                print('step{}/{} (epoch {:.2e}, loss_average = {:.2e}'.format(learning_rate,loss,average))\n",
    "                mse, loss = self.evaluate(val_data,val_labels,sess)\n",
    "                accuracies.append(mse)\n",
    "                print(' validation {}'.format(string))\n",
    "                print(' time: {:.0f}s(wall{:.0f})s'.format(time.process_time() - t_process, time.time() - t_wall))\n",
    "\n",
    "                #Summaries for tensorboard\n",
    "                summary = tf.Summary()\n",
    "                summary.ParseFromString(sess.run(self.op_summary, feed_dict))\n",
    "                summary.value.add(tag = 'validation/accuracy', simple_value = accuracy)\n",
    "                summary.value.add(tag='validation/loss',simple_value = loss)\n",
    "                write.add_summary(summary, step)\n",
    "\n",
    "                #save model parameters for evaluation\n",
    "                self.op_saver.save(sess,path,global_step = step)\n",
    "        print('validation accuracy: peak = {:.2f}, mean={:.2f}'.format(max(accuracies),np.mean(accuracies[-10:])))\n",
    "        writer.close()\n",
    "        sess.close()\n",
    "        \n",
    "        t_step = (time.time() - t_wall) / num_steps\n",
    "        return accuracies, losses, t_step\n",
    "            \n",
    "            \n",
    "    def evaluate(self, data, labels, sess = None):\n",
    "        #return mse and loss\n",
    "        return _,_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 layer fully convolutional network\n",
    "class fc1(base_model):\n",
    "    def __init__(self):\n",
    "        super.__init__()\n",
    "    def _inference(self,x,dropout):\n",
    "        W = self._weight_variable([NFEATURES,NCLASSES])\n",
    "        b = self._bias_variabl([NCLASSES])\n",
    "        y = tf.matmaul(x,W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolutional\n",
    "class cnn2(base_model):\n",
    "    #simple convolutional model\n",
    "    def __init__(self,K,F):\n",
    "        super().__init__()\n",
    "        self.K = K  #PATCH SIZE\n",
    "        self.F = F  # NUMBER OF FEATURES\n",
    "    \n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('conv1'):\n",
    "            W = self._weight_varible([self.K, self.K,1,self.F])\n",
    "            b = self._bias_variable([self.F])\n",
    "            x_2d = tf.reshape(x,[-1,28,28,1])\n",
    "            y_2d = self._conv2d(x_2d,W)+b\n",
    "            y_2d = tf.nn.relu(y_2d)\n",
    "        with tf.name_scope('fc1'):\n",
    "            y = tf.reshape(y_2d, [-1,NFEATURES*self.F])\n",
    "            W = self._weight_variable([NFEATURES*self.F,NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y,W) + b\n",
    "        return y        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier(L, algo = 'eigh', k = 1):\n",
    "    #return the fourier basis, i.e. the EVD of the laplacian\n",
    "    def sort(lamb, U):\n",
    "        idx = lamb.argsort()\n",
    "        return lamb[idx], U[:,idx]\n",
    "    if algo is 'eigh':\n",
    "        lamb,U = np.linalg.eigh(L.toarray())\n",
    "    return lamb, U    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fgcnn2(base_model):\n",
    "    def __init__(self,L,F):\n",
    "        super().__init__()\n",
    "        self.F = F\n",
    "        _, self.U = graph.fourier(L)\n",
    "        \n",
    "    def _inference(self, x, dropout):\n",
    "        #x : NSAMPLES x NFEATURES\n",
    "        with tf.name_scope('gconv1'):\n",
    "            #Transform to Fourier domain\n",
    "            U = tf.constant(self.U, dtype = tf.float32)\n",
    "            xf = tf.matmul(x,U)\n",
    "            xf = tf.expand_dims(xf,1)\n",
    "            xf = tf.transpose(xf)\n",
    "            #Filter\n",
    "            W = self._weight_variable([NFEATURES,self.F,1])\n",
    "            yf = tf.matmul(W,xf)\n",
    "            yf = tf.transpose(yf)\n",
    "            yf = tf.reshape(yf,[-1,NFEATURES])\n",
    "            #Transform back to graph domain\n",
    "            Ut = tf.transpose(U)\n",
    "            y = tf.matmul(yf, Ut)\n",
    "            y = tf.reshape(yf,[-1,self.F,NFEATURES])\n",
    "            #Bias and non-linearity\n",
    "            b = self._bias_variable([1,self.F,1])\n",
    "            y+=b\n",
    "            y = tf.nn.relu(y)\n",
    "            \n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*NFETURES, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1,self.F*FEATURES])\n",
    "            y = tf.matmul(y,W)+b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showTime(t):\n",
    "    tt = time.gmtime(t/1000)\n",
    "    print('{}.{} {}:{}'.format(tt.tm_mon,tt.tm_mday,tt.tm_hour,tt.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 9:20\n"
     ]
    }
   ],
   "source": [
    "showTime(1383297600000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

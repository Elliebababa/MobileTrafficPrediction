{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, RepeatVector, Dense, Activation, Add, Reshape, Input, Lambda, Multiply, Concatenate, Dot, Permute\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K \n",
    "from keras.engine.topology  import Layer\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data and parameter\n",
    "intervals = 144\n",
    "daytest = 7\n",
    "testlen = intervals*daytest\n",
    "stepahead = 48\n",
    "f1 = h5.File('../data/processed/internet_t10_s3030_4070.h5','r')\n",
    "f2 = h5.File('../data/processed/sms_t10_s3030_4070.h5','r')\n",
    "f3 = h5.File('../data/processed/call_t10_s3030_4070.h5','r')\n",
    "data1 = f1['data'].value\n",
    "data2 = f2['data'].value\n",
    "data3 = f3['data'].value\n",
    "dataX = np.concatenate([data1,data2,data3], axis = 1).reshape((8928,5,900))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path,T):\n",
    "    input_X = []\n",
    "    input_Y = []\n",
    "    label_Y = []\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    row_length = len(df)\n",
    "    column_length = df.columns.size\n",
    "    for i in range(row_length-T+1):\n",
    "        X_data = df.iloc[i:i+T, 0:column_length-1]\n",
    "        Y_data = df.iloc[i:i+T-1,column_length-1]\n",
    "        label_data = df.iloc[i+T-1,column_length-1]\n",
    "        input_X.append(np.array(X_data))\n",
    "        input_Y.append(np.array(Y_data))\n",
    "        label_Y.append(np.array(label_data))\n",
    "    input_X = np.array(input_X).reshape(-1,T,n)\n",
    "    input_Y = np.array(input_Y).reshape(-1,T-1,1)\n",
    "    label_Y = np.array(label_Y).reshape(-1,1)\n",
    "\n",
    "    return input_X,input_Y,label_Y\n",
    " \n",
    "input_X_train, input_X_test, input_Y_train,input_Y_test,label_Y_train,label_Y_test = train_test_split(input_X,input_Y,label_Y, test_size=0.3, random_state=0)\n",
    "print('input_X_train shape:',input_X_train.shape)\n",
    "print('input_X_test shape:', input_X_test.shape)\n",
    "print('input_Y_train shape:', input_Y_train.shape)\n",
    "print('input_Y_test shape:',input_Y_test.shape)\n",
    "print('label_Y_train shape:', label_Y_train.shape)\n",
    "print('label_Y_test shape:', label_Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual stage attention\n",
    "\n",
    "reference:\n",
    "- 《A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction》\n",
    "- https://github.com/chensvm/A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction/blob/master/DARNN/dual_attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 6\n",
    "n = 1\n",
    "n_h = 32 #hidden units for LSTMcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  one_encoder_attention_step(h_prev, s_prev, X):\n",
    "    #generate weight of each feature at time t, given previous state and T steps of X\n",
    "    #parma h_prev: previous hidden state, shape = [1, m], 1 represents the last state, m represents the dimension of hidden layer\n",
    "    #parma s_prev:previous cell state, shape = [1, m], 1 represents the last state, m represents the dimension of hidden layer\n",
    "    #X:(n, T) n is the dimension of input series at time t, T is the length of time series\n",
    "    #return:x_t's attention weights, n dimension\n",
    "    concat = Concatenate()([h_prev, s_prev]) # none, 1, 2m\n",
    "    result1 = Dense(T,activation=None, use_bias=False)(concat) #none, 1, T\n",
    "    result1 = RepeatVector(X.shape[1])(result1) #none, n , T\n",
    "    result2 = Dense(T,activation=None, use_bias=False)(X)# none, n, T  Ue (T, T) -> none, n, T \n",
    "    result3 = Add()([result1, result2])#none, n, T\n",
    "    result4 = Activation(activatin = 'tanh')(result3)\n",
    "    result5 = Dense(1)(result4)#none, n, 1\n",
    "    result5 = Permute(axis = (2,1))(result5) #none, 1 , n\n",
    "    alphas = Activation(activation = 'softmax')(result5)\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_attention(T, X, s0, h0):\n",
    "    #X none, n, T\n",
    "    s = s0\n",
    "    h = h0\n",
    "    print('s:',s)\n",
    "    #initialize empty list of outputs\n",
    "    attention_weight_t = None\n",
    "    X_ = None\n",
    "    for t in range(T):\n",
    "        context = one_encoder_attention_step(h,s,X) #softmax weight     none, 1, n\n",
    "        x = Lambda(lambda x: X[:,:,t])(X) #x (none, n), n features at time t\n",
    "        x = Reshape(1,n)(x)\n",
    "        x_ = Multiply()([context,x]) #none, 1, n\n",
    "        h, _, s = LSTM(n_h, return_state = True)(x_, initial_state= [h, s])\n",
    "        if t != 0:\n",
    "            attentin_weight_t =  Concatenate(1)(attention_weight_t, context) #none, T, n\n",
    "            X_ = Concatenate(1)(X_, x_) #none, T, n\n",
    "        else:\n",
    "            attention_weight_t = context\n",
    "            X_ = x_\n",
    "    attention_weight_t = Permute(axis = (2,1))(attention_weight_t)\n",
    "    X_ = Permute(axis = (2,1))(X_)\n",
    "    return X_, attention_weight_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_decoder_attention_step(h_de_prev, s_de_prev, h_en_all):\n",
    "    #param h_prev: previous hidden state\n",
    "    #params s_prev: previous cell state\n",
    "    #param h_en_all: none, T, m  n is length of input series at time t, T is length of time series\n",
    "    #return: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*- coding: utf-8 -*-\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Input,LSTM,Add\n",
    "from keras.models import Model\n",
    "class My_Dot(Layer):  #参数为(inputs,output_dim)  只改变输入的最后一个维度\n",
    "    def __init__(self,output_dim,**kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(My_Dot, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',shape=(input_shape[-1],self.output_dim),\n",
    "                                      initializer='uniform',trainable=True)\n",
    "        super(My_Dot, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        print('Mydot:',K.dot(inputs,self.kernel))\n",
    "        return K.dot(inputs,self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[1],self.output_dim)\n",
    "\n",
    "class My_Transpose(Layer):  #参数为(inputs,axis)  改变维度\n",
    "    def __init__(self,axis,**kwargs):\n",
    "        self.axis = axis\n",
    "        super(My_Transpose, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        super(My_Transpose, self).build(input_shape)\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.permute_dimensions(inputs,pattern=self.axis)\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[self.axis[0]],input_shape[self.axis[1]],input_shape[self.axis[2]])\n",
    "    \n",
    "from keras.layers import LSTM,RepeatVector,Dense,Activation,Add,Reshape,Input,Lambda,Multiply,Concatenate,Dot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "T = 6  #时间序列长度\n",
    "n = 5\n",
    "m = n_h = n_s = 20  #length of hidden state m\n",
    "p = n_hde0 = n_sde0 = 30 \n",
    "epochs = 100\n",
    "\n",
    "en_densor_We = Dense(T)\n",
    "en_LSTM_cell = LSTM(n_h,return_state=True)\n",
    "de_LSTM_cell = LSTM(p,return_state=True)\n",
    "de_densor_We = Dense(m)\n",
    "LSTM_cell = LSTM(p,return_state=True)\n",
    "\n",
    "def one_encoder_attention_step(h_prev,s_prev,X):\n",
    "    '''\n",
    "    :param h_prev: previous hidden state\n",
    "    :param s_prev: previous cell state\n",
    "    :param X: (T,n),n is length of input series at time t,T is length of time series\n",
    "    :return: x_t's attention weights,total n numbers,sum these are 1\n",
    "    '''\n",
    "    concat = Concatenate()([h_prev,s_prev])  #(none,1,2m)\n",
    "    result1 = en_densor_We(concat)   #(none,1,T)\n",
    "    result1 = RepeatVector(X.shape[2],)(result1)  #(none,n,T)\n",
    "    X_temp = My_Transpose(axis=(0,2,1))(X) #X_temp(None,n,T)\n",
    "    result2 = My_Dot(T)(X_temp)  # (none,n,T)  Ue(T,T)\n",
    "    result3 = Add()([result1,result2])  #(none,n,T)\n",
    "    result4 = Activation(activation='tanh')(result3)  #(none,n,T)\n",
    "    result5 = My_Dot(1)(result4)\n",
    "    result5 = My_Transpose(axis=(0,2,1))(result5)\n",
    "    alphas = Activation(activation='softmax')(result5)\n",
    "\n",
    "    return alphas\n",
    "\n",
    "def encoder_attention(T,X,s0,h0):\n",
    "\n",
    "    s = s0\n",
    "    h = h0\n",
    "    print('s:', s)\n",
    "    #initialize empty list of outputs\n",
    "    attention_weight_t = None\n",
    "    for t in range(T):\n",
    "        print('X:', X)\n",
    "        context = one_encoder_attention_step(h,s,X)  #(none,1,n)\n",
    "        print('context:',context)\n",
    "        x = Lambda(lambda x: X[:,t,:])(X)\n",
    "        x = Reshape((1,n))(x)\n",
    "        print('x:',x)\n",
    "        h, _, s = en_LSTM_cell(x, initial_state=[h, s])\n",
    "        context = Reshape([1,n])(context)\n",
    "        if t!=0:\n",
    "            print('attention_weight_t:',attention_weight_t)\n",
    "            attention_weight_t=Concatenate(axis = 1)([attention_weight_t,context])\n",
    "            print('hello')\n",
    "        else:\n",
    "            attention_weight_t = context\n",
    "        print('h:', h)\n",
    "        print('_:', _)\n",
    "        print('s:', s)\n",
    "        print('t', t)\n",
    "        # break\n",
    "\n",
    "    X_ = Multiply()([attention_weight_t,X])\n",
    "    print('return X:',X_)\n",
    "    return X_\n",
    "\n",
    "def one_decoder_attention_step(h_de_prev,s_de_prev,h_en_all):\n",
    "    '''\n",
    "    :param h_prev: previous hidden state\n",
    "    :param s_prev: previous cell state\n",
    "    :param h_en_all: (None,T,m),n is length of input series at time t,T is length of time series\n",
    "    :return: x_t's attention weights,total n numbers,sum these are 1\n",
    "    '''\n",
    "    print('h_en_all:',h_en_all)\n",
    "    concat = Concatenate()([h_de_prev,s_de_prev])  #(None,1,2p)\n",
    "    result1 = de_densor_We(concat)   #(None,1,m)\n",
    "    result1 = RepeatVector(T)(result1)  #(None,T,m)\n",
    "    result2 = My_Dot(m)(h_en_all)\n",
    "    print('result2:',result2)\n",
    "    print('result1:',result1)\n",
    "    result3 = Add()([result1,result2])  #(None,T,m)\n",
    "    result4 = Activation(activation='tanh')(result3)  #(None,T,m)\n",
    "    result5 = My_Dot(1)(result4)\n",
    "\n",
    "    beta = Activation(activation='softmax')(result5)\n",
    "    context = Dot(axes = 1)([beta,h_en_all])  #(1,m)\n",
    "    return context\n",
    "\n",
    "def decoder_attention(T,h_en_all,Y,s0,h0):\n",
    "    s = s0\n",
    "    h = h0\n",
    "    for t in range(T-1):\n",
    "        y_prev = Lambda(lambda y_prev: Y[:, t, :])(Y)\n",
    "        y_prev = Reshape((1, 1))(y_prev)   #(None,1,1)\n",
    "        print('y_prev:',y_prev)\n",
    "        context = one_decoder_attention_step(h,s,h_en_all)  #(None,1,20)\n",
    "        y_prev = Concatenate(axis=2)([y_prev,context])   #(None,1,21)\n",
    "        print('y_prev:',y_prev)\n",
    "        y_prev = Dense(1)(y_prev)       #(None,1,1)\n",
    "        print('y_prev:',y_prev)\n",
    "        h, _, s = de_LSTM_cell(y_prev, initial_state=[h, s])\n",
    "        print('h:', h)\n",
    "        print('_:', _)\n",
    "        print('s:', s)\n",
    "\n",
    "    context = one_decoder_attention_step(h, s, h_en_all)\n",
    "    return h,context\n",
    "\n",
    "X = Input(shape=(T,n))   #输入时间序列数据\n",
    "s0 = Input(shape=(n_s,))  #initialize the first cell state\n",
    "h0 = Input(shape=(n_h,))   #initialize the first hidden state\n",
    "h_de0 = Input(shape=(n_hde0,))\n",
    "s_de0 = Input(shape=(n_sde0,))\n",
    "Y = Input(shape=(T-1,1))\n",
    "X_ = encoder_attention(T,X,s0,h0)\n",
    "print('X_:',X_)\n",
    "X_ = Reshape((T,n))(X_)\n",
    "print('X_:',X_)\n",
    "h_en_all = LSTM(m,return_sequences=True)(X_)\n",
    "h_en_all = Reshape((T,-1))(h_en_all)\n",
    "print('h_en_all:',h_en_all)\n",
    "\n",
    "h,context = decoder_attention(T,h_en_all,Y,s_de0,h_de0)\n",
    "h = Reshape((1,p))(h)\n",
    "concat = Concatenate(axis=2)([h,context])\n",
    "concat = Reshape((-1,))(concat)\n",
    "print('concat:',concat)\n",
    "result = Dense(p)(concat)\n",
    "print('result:',result)\n",
    "output = Dense(1)(result)\n",
    "\n",
    "s0_train = h0_train = np.zeros((input_X_train.shape[0],m))\n",
    "h_de0_train = s_de0_train =np.zeros((input_X_train.shape[0],p))\n",
    "model = Model(inputs=[X,Y,s0,h0,s_de0,h_de0],outputs=output)\n",
    "model.compile(loss='mse',optimizer='adam',metrics=['mse'])\n",
    "model.summary()\n",
    "# early_stopping = EarlyStopping(monitor='val_mean_squared_error',patience=20,mode='min')\n",
    "# model_checkpoint = ModelCheckpoint(fname_model,monitor='val_mean_squared_error',verbose=0)\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_{}.png'.format('using_origin'), show_shapes = True)\n",
    "model.fit([input_X_train,input_Y_train,s0_train,h0_train,s_de0_train,h_de0_train],label_Y_train,epochs=epochs,batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "s0_test = h0_test = np.zeros((input_X_test.shape[0],m))\n",
    "h_de0_test = s_de0_test =np.zeros((input_X_test.shape[0],p))\n",
    "score = model.evaluate([input_X_test,input_Y_test,s0_test,h0_test,s_de0_test,h_de0_test],label_Y_test,batch_size=input_X_test.shape[0],verbose=1)\n",
    "print('loss:',score[0])\n",
    "print('mse:',score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
